# ============================================================================
# vLLM Inference Server - Environment Variables
# ============================================================================
# Copy this file to .env or .env.local and fill in your values
# Usage: source .env (or use python-dotenv to load automatically)

# ============================================================================
# MODAL CONFIGURATION
# ============================================================================
# Modal is used for cloud deployment. You need to authenticate with Modal CLI.
# Run: modal token new
# The token is stored in ~/.modal.toml automatically

# ============================================================================
# HUGGING FACE CONFIGURATION
# ============================================================================
# HuggingFace token for downloading models (especially gated/private models)
# Get your token from: https://huggingface.co/settings/tokens
# Required for: Llama, Mistral gated models, private models
# Optional for: Public models like Qwen
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Enable faster downloads (already set in vllm_inference.py, but can be overridden)
HF_HUB_ENABLE_HF_TRANSFER=1

# ============================================================================
# VLLM SERVER CONFIGURATION (For Deployment)
# ============================================================================

# Model Configuration
# Model from HuggingFace Hub (any vLLM-compatible model)
VLLM_MODEL_NAME=Qwen/Qwen3-0.6B

# Optional: Specific model revision/branch (default: main/latest)
# VLLM_MODEL_REVISION=main

# GPU Configuration
# Number of H100 GPUs to use (1-8, depending on model size)
VLLM_N_GPU=1

# Performance Settings
# FAST_BOOT: true = faster startup (1-2 min), false = maximum performance
VLLM_FAST_BOOT=true

# Server port (default: 8000)
VLLM_PORT=8000

# Scaling Configuration
# Minutes to keep server alive after last request (balance cost vs cold starts)
VLLM_SCALEDOWN_MINUTES=10

# Container startup timeout in minutes
VLLM_TIMEOUT_MINUTES=10

# ============================================================================
# CLIENT CONFIGURATION (For client_example.py)
# ============================================================================

# Base URL of your deployed vLLM server
# After deploying, you'll get a URL like:
# https://your-workspace--vllm-qwen-inference-serve.modal.run
VLLM_BASE_URL=https://your-workspace--vllm-qwen-inference-serve.modal.run

# Model name to use in API requests
# VLLM_MODEL_NAME=Qwen/Qwen3-0.6B

# ============================================================================
# ALTERNATIVE MODELS (Examples)
# ============================================================================
# Uncomment and set VLLM_MODEL_NAME to use a different model

# Qwen Series (Recommended - Best for beginners)
# VLLM_MODEL_NAME=Qwen/Qwen3-0.6B    # 0.6B params - Very fast, H100 x1
# VLLM_MODEL_NAME=Qwen/Qwen3-1B      # 1B params - Fast, H100 x1
# VLLM_MODEL_NAME=Qwen/Qwen3-3B      # 3B params - Good balance, H100 x1
# VLLM_MODEL_NAME=Qwen/Qwen3-7B      # 7B params - Better quality, H100 x1-2

# Mistral AI (Requires HF_TOKEN for some versions)
# VLLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.1
# VLLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3

# Meta Llama (Requires HF_TOKEN and approval)
# VLLM_MODEL_NAME=meta-llama/Llama-2-7b-hf
# VLLM_MODEL_NAME=meta-llama/Llama-2-13b-hf
# VLLM_MODEL_NAME=meta-llama/Meta-Llama-3-8B

# Other Popular Models
# VLLM_MODEL_NAME=tiiuae/falcon-7b-instruct
# VLLM_MODEL_NAME=databricks/dolly-v2-7b
# VLLM_MODEL_NAME=mosaicml/mpt-7b-instruct

# ============================================================================
# AUTHENTICATION & SECURITY
# ============================================================================

# For gated models (Llama, Mistral):
# 1. Go to model page on HuggingFace
# 2. Request access
# 3. Generate HF token: https://huggingface.co/settings/tokens
# 4. Set HF_TOKEN above

# For Modal authentication:
# 1. Run: modal token new
# 2. Token stored automatically in ~/.modal.toml

# ============================================================================
# NOTES
# ============================================================================
# - Modal authentication is done via CLI: modal token new
# - HF_TOKEN is only required for gated/private models
# - All VLLM_* variables can be set inline during deployment:
#   VLLM_MODEL_NAME="Qwen/Qwen3-1B" modal deploy vllm_inference.py
